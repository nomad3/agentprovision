"""Data discovery and querying tools.

All data operations route through MCP server to Databricks.
"""
from typing import Optional

from services.databricks_client import get_databricks_client


async def discover_datasets(
    tenant_id: str,
    search_query: str = "",
) -> list[dict]:
    """Find available datasets matching criteria.

    Args:
        tenant_id: Tenant context for isolation
        search_query: Optional natural language search (e.g., "sales data from 2024")

    Returns:
        List of datasets with name, schema, row count, last updated
    """
    client = get_databricks_client()
    # List tables from tenant's catalog
    catalog = f"tenant_{tenant_id.replace('-', '_')}"
    tables = await client.list_tables(catalog=catalog, schema="silver")

    # Filter by search query if provided
    if search_query:
        search_lower = search_query.lower()
        tables = [t for t in tables if search_lower in t.get("name", "").lower()]

    return tables


async def get_dataset_schema(dataset_id: str) -> dict:
    """Get detailed schema with column types, nullability, sample values.

    Args:
        dataset_id: Dataset identifier (format: catalog.schema.table)

    Returns:
        Schema with columns, types, and sample data
    """
    client = get_databricks_client()
    parts = dataset_id.split(".")
    if len(parts) != 3:
        return {"error": "Invalid dataset_id format. Expected: catalog.schema.table"}

    catalog, schema, table = parts
    return await client.describe_table(catalog=catalog, schema=schema, table=table)


async def get_dataset_statistics(dataset_id: str) -> dict:
    """Get statistical profile: distributions, correlations, anomalies.

    Args:
        dataset_id: Dataset identifier (format: catalog.schema.table)

    Returns:
        Statistical summary including counts, means, distributions
    """
    client = get_databricks_client()

    # Run DESCRIBE EXTENDED to get table stats
    sql = f"DESCRIBE EXTENDED {dataset_id}"
    result = await client.query_sql(sql=sql)

    # Also get row count and column stats
    count_sql = f"SELECT COUNT(*) as row_count FROM {dataset_id}"
    count_result = await client.query_sql(sql=count_sql)

    return {
        "table_info": result,
        "row_count": count_result.get("rows", [{}])[0].get("row_count", 0),
    }


async def query_sql(
    sql: str,
    explanation: str = "",
    limit: int = 1000,
) -> dict:
    """Execute SQL query on Databricks Unity Catalog.

    Args:
        sql: The SQL query to execute
        explanation: Brief explanation of what this query does
        limit: Maximum rows to return (default 1000)

    Returns:
        Query results with rows, column names, and metadata
    """
    client = get_databricks_client()

    # Add LIMIT if not present
    sql_upper = sql.upper()
    if "LIMIT" not in sql_upper:
        sql = f"{sql.rstrip(';')} LIMIT {limit}"

    result = await client.query_sql(sql=sql, limit=limit)

    return {
        "rows": result.get("rows", []),
        "columns": result.get("columns", []),
        "row_count": len(result.get("rows", [])),
        "explanation": explanation,
        "query": sql,
    }


async def query_natural_language(
    question: str,
    dataset_ids: list[str],
) -> dict:
    """Convert natural language question to SQL and execute.

    Args:
        question: Natural language question (e.g., "What were top 10 products by revenue?")
        dataset_ids: List of dataset identifiers to query against

    Returns:
        Generated SQL, results, and explanation
    """
    # This will be enhanced with LLM-powered SQL generation
    # For now, return a placeholder that the agent can work with
    return {
        "question": question,
        "datasets": dataset_ids,
        "note": "Natural language query requires agent to generate SQL based on schema",
    }


async def generate_insights(
    dataset_id: str,
    focus_areas: Optional[list[str]] = None,
) -> dict:
    """Auto-generate insights from dataset.

    Args:
        dataset_id: Dataset identifier
        focus_areas: Optional list of areas to focus on (e.g., ["trends", "anomalies"])

    Returns:
        Key findings, suggested follow-up questions, visualization recommendations
    """
    client = get_databricks_client()

    # Get basic statistics
    stats_sql = f"""
    SELECT
        COUNT(*) as total_rows,
        COUNT(DISTINCT *) as unique_rows
    FROM {dataset_id}
    """
    stats = await client.query_sql(sql=stats_sql)

    return {
        "dataset": dataset_id,
        "statistics": stats,
        "focus_areas": focus_areas or ["general"],
        "note": "Detailed insights will be generated by the agent based on data exploration",
    }
